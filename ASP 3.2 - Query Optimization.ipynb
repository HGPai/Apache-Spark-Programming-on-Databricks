{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df6aacce-8d74-4a03-8812-18287fb94f3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Query Optimization\n",
    "\n",
    "We'll explore query plans and optimizations for several examples including logical optimizations and exanples with and without predicate pushdown.\n",
    "\n",
    "##### Objectives\n",
    "1. Logical optimizations\n",
    "1. Predicate pushdown\n",
    "1. No predicate pushdown\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>: `explain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49ae2c5e-232a-4e3c-8039-7ec78bb5724c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![query optimization](https://files.training.databricks.com/images/aspwd/query_optimization_catalyst.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9d3ae84-5067-447c-9431-d3d235ac5441",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![query optimization aqe](https://files.training.databricks.com/images/aspwd/query_optimization_aqe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e8e9709-f847-487b-9bc4-8714a9595db6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Letâ€™s run our set up cell, and get our initial DataFrame stored in the variable `df`. Displaying this DataFrame shows us events data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70c5904d-3b55-451d-a607-eb700515601c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finished setting up utiltity methods..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Finished setting up utiltity methods...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Datasets mounted and student environment set up"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Datasets mounted and student environment set up",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eb0502c-6d8d-48f3-b33f-1e62ac5f682f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(eventsPath)\n",
    "#display(df.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbff94da-49dd-452d-9824-dbb2b0d96818",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Logical Optimization\n",
    "\n",
    "`explain(..)` prints the query plans, optionally formatted by a given explain mode. Compare the following logical plan & physical plan, noting how Catalyst handled the multiple `filter` transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e3a28d5-05a1-4864-bbdd-22cbbd6f35a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "== Parsed Logical Plan ==\n",
       "'Filter NOT ('event_name = press)\n",
       "+- Filter NOT (event_name#224 = shipping_info)\n",
       "   +- Filter NOT (event_name#224 = delivery)\n",
       "      +- Filter NOT (event_name#224 = cc_info)\n",
       "         +- Filter NOT (event_name#224 = email_coupon)\n",
       "            +- Filter NOT (event_name#224 = register)\n",
       "               +- Filter NOT (event_name#224 = checkout)\n",
       "                  +- Filter NOT (event_name#224 = reviews)\n",
       "                     +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n",
       "\n",
       "== Analyzed Logical Plan ==\n",
       "device: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string\n",
       "Filter NOT (event_name#224 = press)\n",
       "+- Filter NOT (event_name#224 = shipping_info)\n",
       "   +- Filter NOT (event_name#224 = delivery)\n",
       "      +- Filter NOT (event_name#224 = cc_info)\n",
       "         +- Filter NOT (event_name#224 = email_coupon)\n",
       "            +- Filter NOT (event_name#224 = register)\n",
       "               +- Filter NOT (event_name#224 = checkout)\n",
       "                  +- Filter NOT (event_name#224 = reviews)\n",
       "                     +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n",
       "\n",
       "== Optimized Logical Plan ==\n",
       "Filter (isnotnull(event_name#224) AND (((NOT (event_name#224 = reviews) AND NOT (event_name#224 = checkout)) AND (NOT (event_name#224 = register) AND NOT (event_name#224 = email_coupon))) AND (((NOT (event_name#224 = cc_info) AND NOT (event_name#224 = delivery)) AND NOT (event_name#224 = shipping_info)) AND NOT (event_name#224 = press))))\n",
       "+- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n",
       "\n",
       "== Physical Plan ==\n",
       "*(1) Filter ((((((((isnotnull(event_name#224) AND NOT (event_name#224 = reviews)) AND NOT (event_name#224 = checkout)) AND NOT (event_name#224 = register)) AND NOT (event_name#224 = email_coupon)) AND NOT (event_name#224 = cc_info)) AND NOT (event_name#224 = delivery)) AND NOT (event_name#224 = shipping_info)) AND NOT (event_name#224 = press))\n",
       "+- *(1) ColumnarToRow\n",
       "   +- FileScan parquet [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] Batched: true, DataFilters: [isnotnull(event_name#224), NOT (event_name#224 = reviews), NOT (event_name#224 = checkout), NOT ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/training/ecommerce/events/events.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(event_name), Not(EqualTo(event_name,reviews)), Not(EqualTo(event_name,checkout)), Not(..., ReadSchema: struct<device:string,ecommerce:struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,u...\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "== Parsed Logical Plan ==\n'Filter NOT ('event_name = press)\n+- Filter NOT (event_name#224 = shipping_info)\n   +- Filter NOT (event_name#224 = delivery)\n      +- Filter NOT (event_name#224 = cc_info)\n         +- Filter NOT (event_name#224 = email_coupon)\n            +- Filter NOT (event_name#224 = register)\n               +- Filter NOT (event_name#224 = checkout)\n                  +- Filter NOT (event_name#224 = reviews)\n                     +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n\n== Analyzed Logical Plan ==\ndevice: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string\nFilter NOT (event_name#224 = press)\n+- Filter NOT (event_name#224 = shipping_info)\n   +- Filter NOT (event_name#224 = delivery)\n      +- Filter NOT (event_name#224 = cc_info)\n         +- Filter NOT (event_name#224 = email_coupon)\n            +- Filter NOT (event_name#224 = register)\n               +- Filter NOT (event_name#224 = checkout)\n                  +- Filter NOT (event_name#224 = reviews)\n                     +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n\n== Optimized Logical Plan ==\nFilter (isnotnull(event_name#224) AND (((NOT (event_name#224 = reviews) AND NOT (event_name#224 = checkout)) AND (NOT (event_name#224 = register) AND NOT (event_name#224 = email_coupon))) AND (((NOT (event_name#224 = cc_info) AND NOT (event_name#224 = delivery)) AND NOT (event_name#224 = shipping_info)) AND NOT (event_name#224 = press))))\n+- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n\n== Physical Plan ==\n*(1) Filter ((((((((isnotnull(event_name#224) AND NOT (event_name#224 = reviews)) AND NOT (event_name#224 = checkout)) AND NOT (event_name#224 = register)) AND NOT (event_name#224 = email_coupon)) AND NOT (event_name#224 = cc_info)) AND NOT (event_name#224 = delivery)) AND NOT (event_name#224 = shipping_info)) AND NOT (event_name#224 = press))\n+- *(1) ColumnarToRow\n   +- FileScan parquet [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] Batched: true, DataFilters: [isnotnull(event_name#224), NOT (event_name#224 = reviews), NOT (event_name#224 = checkout), NOT ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/training/ecommerce/events/events.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(event_name), Not(EqualTo(event_name,reviews)), Not(EqualTo(event_name,checkout)), Not(..., ReadSchema: struct<device:string,ecommerce:struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,u...\n\n",
       "datasetInfos": [],
       "metadata": {},
       "name": null,
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "limitEventsDF = (df\n",
    "                 .filter(col(\"event_name\") != \"reviews\")\n",
    "                 .filter(col(\"event_name\") != \"checkout\")\n",
    "                 .filter(col(\"event_name\") != \"register\")\n",
    "                 .filter(col(\"event_name\") != \"email_coupon\")\n",
    "                 .filter(col(\"event_name\") != \"cc_info\")\n",
    "                 .filter(col(\"event_name\") != \"delivery\")\n",
    "                 .filter(col(\"event_name\") != \"shipping_info\")\n",
    "                 .filter(col(\"event_name\") != \"press\")\n",
    "                )\n",
    "\n",
    "limitEventsDF.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49d2844c-f4dc-4ba3-86de-df7254d30a15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "== Parsed Logical Plan ==\n",
       "'Aggregate ['geo.state], ['geo.state, count(1) AS count#455L]\n",
       "+- Filter isnotnull(ecommerce#223.purchase_revenue_in_usd)\n",
       "   +- Filter isnotnull(ecommerce#223.total_item_quantity)\n",
       "      +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n",
       "\n",
       "== Analyzed Logical Plan ==\n",
       "state: string, count: bigint\n",
       "Aggregate [geo#227.state], [geo#227.state AS state#457, count(1) AS count#455L]\n",
       "+- Filter isnotnull(ecommerce#223.purchase_revenue_in_usd)\n",
       "   +- Filter isnotnull(ecommerce#223.total_item_quantity)\n",
       "      +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n",
       "\n",
       "== Optimized Logical Plan ==\n",
       "Aggregate [_groupingexpression#460], [_groupingexpression#460 AS state#457, count(1) AS count#455L]\n",
       "+- Project [geo#227.state AS _groupingexpression#460]\n",
       "   +- Filter (isnotnull(ecommerce#223.total_item_quantity) AND isnotnull(ecommerce#223.purchase_revenue_in_usd))\n",
       "      +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n",
       "\n",
       "== Physical Plan ==\n",
       "AdaptiveSparkPlan isFinalPlan=false\n",
       "+- HashAggregate(keys=[_groupingexpression#460], functions=[finalmerge_count(merge count#472L) AS count(1)#454L], output=[state#457, count#455L])\n",
       "   +- Exchange hashpartitioning(_groupingexpression#460, 200), ENSURE_REQUIREMENTS, [plan_id=371]\n",
       "      +- HashAggregate(keys=[_groupingexpression#460], functions=[partial_count(1) AS count#472L], output=[_groupingexpression#460, count#472L])\n",
       "         +- Project [geo#227.state AS _groupingexpression#460]\n",
       "            +- Filter (isnotnull(ecommerce#223.total_item_quantity) AND isnotnull(ecommerce#223.purchase_revenue_in_usd))\n",
       "               +- FileScan parquet [ecommerce#223,geo#227] Batched: true, DataFilters: [isnotnull(ecommerce#223.total_item_quantity), isnotnull(ecommerce#223.purchase_revenue_in_usd)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/training/ecommerce/events/events.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(ecommerce.total_item_quantity), IsNotNull(ecommerce.purchase_revenue_in_usd)], ReadSchema: struct<ecommerce:struct<purchase_revenue_in_usd:double,total_item_quantity:bigint>,geo:struct<sta...\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "== Parsed Logical Plan ==\n'Aggregate ['geo.state], ['geo.state, count(1) AS count#455L]\n+- Filter isnotnull(ecommerce#223.purchase_revenue_in_usd)\n   +- Filter isnotnull(ecommerce#223.total_item_quantity)\n      +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n\n== Analyzed Logical Plan ==\nstate: string, count: bigint\nAggregate [geo#227.state], [geo#227.state AS state#457, count(1) AS count#455L]\n+- Filter isnotnull(ecommerce#223.purchase_revenue_in_usd)\n   +- Filter isnotnull(ecommerce#223.total_item_quantity)\n      +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n\n== Optimized Logical Plan ==\nAggregate [_groupingexpression#460], [_groupingexpression#460 AS state#457, count(1) AS count#455L]\n+- Project [geo#227.state AS _groupingexpression#460]\n   +- Filter (isnotnull(ecommerce#223.total_item_quantity) AND isnotnull(ecommerce#223.purchase_revenue_in_usd))\n      +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[_groupingexpression#460], functions=[finalmerge_count(merge count#472L) AS count(1)#454L], output=[state#457, count#455L])\n   +- Exchange hashpartitioning(_groupingexpression#460, 200), ENSURE_REQUIREMENTS, [plan_id=371]\n      +- HashAggregate(keys=[_groupingexpression#460], functions=[partial_count(1) AS count#472L], output=[_groupingexpression#460, count#472L])\n         +- Project [geo#227.state AS _groupingexpression#460]\n            +- Filter (isnotnull(ecommerce#223.total_item_quantity) AND isnotnull(ecommerce#223.purchase_revenue_in_usd))\n               +- FileScan parquet [ecommerce#223,geo#227] Batched: true, DataFilters: [isnotnull(ecommerce#223.total_item_quantity), isnotnull(ecommerce#223.purchase_revenue_in_usd)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/training/ecommerce/events/events.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(ecommerce.total_item_quantity), IsNotNull(ecommerce.purchase_revenue_in_usd)], ReadSchema: struct<ecommerce:struct<purchase_revenue_in_usd:double,total_item_quantity:bigint>,geo:struct<sta...\n\n",
       "datasetInfos": [],
       "metadata": {},
       "name": null,
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df.filter(col(\"ecommerce.total_item_quantity\").isNotNull())\n",
    "   .filter(col(\"ecommerce.purchase_revenue_in_usd\").isNotNull())\n",
    "  .groupBy(\"geo.state\")\n",
    "  .count()).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbf6506d-e914-4045-bdb5-187d4af8e731",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Of course, we could have written the query originally using a single `filter` condition ourselves. Compare the previous and following query plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4904d0d6-b84b-4cbd-846a-4b70f0e31787",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "== Parsed Logical Plan ==\n",
       "'Filter ((((((((isnotnull('event_name) AND NOT ('event_name = reviews)) AND NOT ('event_name = checkout)) AND NOT ('event_name = register)) AND NOT ('event_name = email_coupon)) AND NOT ('event_name = cc_info)) AND NOT ('event_name = delivery)) AND NOT ('event_name = shipping_info)) AND NOT ('event_name = press))\n",
       "+- Relation [device#159,ecommerce#160,event_name#161,event_previous_timestamp#162L,event_timestamp#163L,geo#164,items#165,traffic_source#166,user_first_touch_timestamp#167L,user_id#168] parquet\n",
       "\n",
       "== Analyzed Logical Plan ==\n",
       "device: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string\n",
       "Filter ((((((((isnotnull(event_name#161) AND NOT (event_name#161 = reviews)) AND NOT (event_name#161 = checkout)) AND NOT (event_name#161 = register)) AND NOT (event_name#161 = email_coupon)) AND NOT (event_name#161 = cc_info)) AND NOT (event_name#161 = delivery)) AND NOT (event_name#161 = shipping_info)) AND NOT (event_name#161 = press))\n",
       "+- Relation [device#159,ecommerce#160,event_name#161,event_previous_timestamp#162L,event_timestamp#163L,geo#164,items#165,traffic_source#166,user_first_touch_timestamp#167L,user_id#168] parquet\n",
       "\n",
       "== Optimized Logical Plan ==\n",
       "Filter ((((((((isnotnull(event_name#161) AND NOT (event_name#161 = reviews)) AND NOT (event_name#161 = checkout)) AND NOT (event_name#161 = register)) AND NOT (event_name#161 = email_coupon)) AND NOT (event_name#161 = cc_info)) AND NOT (event_name#161 = delivery)) AND NOT (event_name#161 = shipping_info)) AND NOT (event_name#161 = press))\n",
       "+- Relation [device#159,ecommerce#160,event_name#161,event_previous_timestamp#162L,event_timestamp#163L,geo#164,items#165,traffic_source#166,user_first_touch_timestamp#167L,user_id#168] parquet\n",
       "\n",
       "== Physical Plan ==\n",
       "*(1) Filter ((((((((isnotnull(event_name#161) AND NOT (event_name#161 = reviews)) AND NOT (event_name#161 = checkout)) AND NOT (event_name#161 = register)) AND NOT (event_name#161 = email_coupon)) AND NOT (event_name#161 = cc_info)) AND NOT (event_name#161 = delivery)) AND NOT (event_name#161 = shipping_info)) AND NOT (event_name#161 = press))\n",
       "+- *(1) ColumnarToRow\n",
       "   +- FileScan parquet [device#159,ecommerce#160,event_name#161,event_previous_timestamp#162L,event_timestamp#163L,geo#164,items#165,traffic_source#166,user_first_touch_timestamp#167L,user_id#168] Batched: true, DataFilters: [isnotnull(event_name#161), NOT (event_name#161 = reviews), NOT (event_name#161 = checkout), NOT ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/training/ecommerce/events/events.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(event_name), Not(EqualTo(event_name,reviews)), Not(EqualTo(event_name,checkout)), Not(..., ReadSchema: struct<device:string,ecommerce:struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,u...\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "== Parsed Logical Plan ==\n'Filter ((((((((isnotnull('event_name) AND NOT ('event_name = reviews)) AND NOT ('event_name = checkout)) AND NOT ('event_name = register)) AND NOT ('event_name = email_coupon)) AND NOT ('event_name = cc_info)) AND NOT ('event_name = delivery)) AND NOT ('event_name = shipping_info)) AND NOT ('event_name = press))\n+- Relation [device#159,ecommerce#160,event_name#161,event_previous_timestamp#162L,event_timestamp#163L,geo#164,items#165,traffic_source#166,user_first_touch_timestamp#167L,user_id#168] parquet\n\n== Analyzed Logical Plan ==\ndevice: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string\nFilter ((((((((isnotnull(event_name#161) AND NOT (event_name#161 = reviews)) AND NOT (event_name#161 = checkout)) AND NOT (event_name#161 = register)) AND NOT (event_name#161 = email_coupon)) AND NOT (event_name#161 = cc_info)) AND NOT (event_name#161 = delivery)) AND NOT (event_name#161 = shipping_info)) AND NOT (event_name#161 = press))\n+- Relation [device#159,ecommerce#160,event_name#161,event_previous_timestamp#162L,event_timestamp#163L,geo#164,items#165,traffic_source#166,user_first_touch_timestamp#167L,user_id#168] parquet\n\n== Optimized Logical Plan ==\nFilter ((((((((isnotnull(event_name#161) AND NOT (event_name#161 = reviews)) AND NOT (event_name#161 = checkout)) AND NOT (event_name#161 = register)) AND NOT (event_name#161 = email_coupon)) AND NOT (event_name#161 = cc_info)) AND NOT (event_name#161 = delivery)) AND NOT (event_name#161 = shipping_info)) AND NOT (event_name#161 = press))\n+- Relation [device#159,ecommerce#160,event_name#161,event_previous_timestamp#162L,event_timestamp#163L,geo#164,items#165,traffic_source#166,user_first_touch_timestamp#167L,user_id#168] parquet\n\n== Physical Plan ==\n*(1) Filter ((((((((isnotnull(event_name#161) AND NOT (event_name#161 = reviews)) AND NOT (event_name#161 = checkout)) AND NOT (event_name#161 = register)) AND NOT (event_name#161 = email_coupon)) AND NOT (event_name#161 = cc_info)) AND NOT (event_name#161 = delivery)) AND NOT (event_name#161 = shipping_info)) AND NOT (event_name#161 = press))\n+- *(1) ColumnarToRow\n   +- FileScan parquet [device#159,ecommerce#160,event_name#161,event_previous_timestamp#162L,event_timestamp#163L,geo#164,items#165,traffic_source#166,user_first_touch_timestamp#167L,user_id#168] Batched: true, DataFilters: [isnotnull(event_name#161), NOT (event_name#161 = reviews), NOT (event_name#161 = checkout), NOT ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/training/ecommerce/events/events.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(event_name), Not(EqualTo(event_name,reviews)), Not(EqualTo(event_name,checkout)), Not(..., ReadSchema: struct<device:string,ecommerce:struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,u...\n\n",
       "datasetInfos": [],
       "metadata": {},
       "name": null,
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "betterDF = (df\n",
    "            .filter((col(\"event_name\").isNotNull()) &\n",
    "                    (col(\"event_name\") != \"reviews\") &\n",
    "                    (col(\"event_name\") != \"checkout\") &\n",
    "                    (col(\"event_name\") != \"register\") &\n",
    "                    (col(\"event_name\") != \"email_coupon\") &\n",
    "                    (col(\"event_name\") != \"cc_info\") &\n",
    "                    (col(\"event_name\") != \"delivery\") &\n",
    "                    (col(\"event_name\") != \"shipping_info\") &\n",
    "                    (col(\"event_name\") != \"press\"))\n",
    "           )\n",
    "\n",
    "betterDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e3b83c7-a090-4136-ad06-633e20c0ad8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Of course, we wouldn't write the following code intentionally, but in a long, complex query you might not notice the duplicate filter conditions. Let's see what Catalyst does with this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c021af63-d399-4828-bca2-02566ed68c0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "== Parsed Logical Plan ==\n",
       "'Filter NOT ('event_name = finalize)\n",
       "+- Filter NOT (event_name#224 = finalize)\n",
       "   +- Filter NOT (event_name#224 = finalize)\n",
       "      +- Filter NOT (event_name#224 = finalize)\n",
       "         +- Filter NOT (event_name#224 = finalize)\n",
       "            +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n",
       "\n",
       "== Analyzed Logical Plan ==\n",
       "device: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string\n",
       "Filter NOT (event_name#224 = finalize)\n",
       "+- Filter NOT (event_name#224 = finalize)\n",
       "   +- Filter NOT (event_name#224 = finalize)\n",
       "      +- Filter NOT (event_name#224 = finalize)\n",
       "         +- Filter NOT (event_name#224 = finalize)\n",
       "            +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n",
       "\n",
       "== Optimized Logical Plan ==\n",
       "Filter (isnotnull(event_name#224) AND NOT (event_name#224 = finalize))\n",
       "+- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n",
       "\n",
       "== Physical Plan ==\n",
       "*(1) Filter (isnotnull(event_name#224) AND NOT (event_name#224 = finalize))\n",
       "+- *(1) ColumnarToRow\n",
       "   +- FileScan parquet [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] Batched: true, DataFilters: [isnotnull(event_name#224), NOT (event_name#224 = finalize)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/training/ecommerce/events/events.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(event_name), Not(EqualTo(event_name,finalize))], ReadSchema: struct<device:string,ecommerce:struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,u...\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "== Parsed Logical Plan ==\n'Filter NOT ('event_name = finalize)\n+- Filter NOT (event_name#224 = finalize)\n   +- Filter NOT (event_name#224 = finalize)\n      +- Filter NOT (event_name#224 = finalize)\n         +- Filter NOT (event_name#224 = finalize)\n            +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n\n== Analyzed Logical Plan ==\ndevice: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string\nFilter NOT (event_name#224 = finalize)\n+- Filter NOT (event_name#224 = finalize)\n   +- Filter NOT (event_name#224 = finalize)\n      +- Filter NOT (event_name#224 = finalize)\n         +- Filter NOT (event_name#224 = finalize)\n            +- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n\n== Optimized Logical Plan ==\nFilter (isnotnull(event_name#224) AND NOT (event_name#224 = finalize))\n+- Relation [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] parquet\n\n== Physical Plan ==\n*(1) Filter (isnotnull(event_name#224) AND NOT (event_name#224 = finalize))\n+- *(1) ColumnarToRow\n   +- FileScan parquet [device#222,ecommerce#223,event_name#224,event_previous_timestamp#225L,event_timestamp#226L,geo#227,items#228,traffic_source#229,user_first_touch_timestamp#230L,user_id#231] Batched: true, DataFilters: [isnotnull(event_name#224), NOT (event_name#224 = finalize)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/training/ecommerce/events/events.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(event_name), Not(EqualTo(event_name,finalize))], ReadSchema: struct<device:string,ecommerce:struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,u...\n\n",
       "datasetInfos": [],
       "metadata": {},
       "name": null,
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stupidDF = (df\n",
    "            .filter(col(\"event_name\") != \"finalize\")\n",
    "            .filter(col(\"event_name\") != \"finalize\")\n",
    "            .filter(col(\"event_name\") != \"finalize\")\n",
    "            .filter(col(\"event_name\") != \"finalize\")\n",
    "            .filter(col(\"event_name\") != \"finalize\")\n",
    "           )\n",
    "\n",
    "stupidDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dfa2823-1df1-457f-a70b-4a420cb98dc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Caching\n",
    "\n",
    "By default the data of a DataFrame is present on a Spark cluster only while it is being processed during a query -- it is not automatically persisted on the cluster afterwards. (Spark is a data processing engine, not a data storage system.) You can explicity request Spark to persist a DataFrame on the cluster by invoking its `cache` method.\n",
    "\n",
    "If you do cache a DataFrame, you should always explictly evict it from cache by invoking `unpersist` when you no longer need it.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_best_32.png\" alt=\"Best Practice\"> Caching a DataFrame can be appropriate if you are certain that you will use the same DataFrame multiple times, as in:\n",
    "\n",
    "- Exploratory data analysis\n",
    "- Machine learning model training\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> Aside from those use cases, you should **not** cache DataFrames because it is likely that you'll *degrade* the performance of your application.\n",
    "\n",
    "- Caching consumes cluster resources that could otherwise be used for task execution\n",
    "- Caching can prevent Spark from performing query optimizations, as shown in the next example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23bf40e9-2511-42a0-8f64-76138ca8c1c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Predicate Pushdown\n",
    "\n",
    "Here is example reading from a JDBC source, where Catalyst determines that *predicate pushdown* can take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3cfc32f-4115-498f-ac59-7f5289d8372f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res3: Class[_] = class org.postgresql.Driver\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res3: Class[_] = class org.postgresql.Driver\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Ensure that the driver class is loaded\n",
    "Class.forName(\"org.postgresql.Driver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcURL = \"jdbc:postgresql://54.213.33.240/training\"\n",
    "\n",
    "# Username and Password w/read-only rights\n",
    "connProperties = {\n",
    "    \"user\" : \"training\",\n",
    "    \"password\" : \"training\"\n",
    "}\n",
    "\n",
    "ppDF = (spark\n",
    "        .read\n",
    "        .jdbc(\n",
    "            url=jdbcURL,                  # the JDBC URL\n",
    "            table=\"training.people_1m\",   # the name of the table\n",
    "            column=\"id\",                  # the name of a column of an integral type that will be used for partitioning\n",
    "            lowerBound=1,                 # the minimum value of columnName used to decide partition stride\n",
    "            upperBound=1000000,           # the maximum value of columnName used to decide partition stride\n",
    "            numPartitions=8,              # the number of partitions/connections\n",
    "            properties=connProperties     # the connection properties\n",
    "        )\n",
    "        .filter(col(\"gender\") == \"M\")   # Filter the data by gender\n",
    "       )\n",
    "\n",
    "ppDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "302a6527-7799-46c0-9c27-e4693ba16418",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note the lack of a **Filter** and the presence of a **PushedFilters** in the **Scan**. The filter operation is pushed to the database and only the matching records are sent to Spark. This can greatly reduce the amount of data that Spark needs to ingest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d654cce4-311a-4103-8319-0c728a3b956c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### No Predicate Pushdown\n",
    "\n",
    "In comparison, caching the data before filtering eliminates the possibility for the predicate push down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedDF = (spark\n",
    "            .read\n",
    "            .jdbc(\n",
    "                url=jdbcURL,\n",
    "                table=\"training.people_1m\",\n",
    "                column=\"id\",\n",
    "                lowerBound=1,\n",
    "                upperBound=1000000,\n",
    "                numPartitions=8,\n",
    "                properties=connProperties\n",
    "            )\n",
    "           )\n",
    "\n",
    "cachedDF.cache()\n",
    "filteredDF = cachedDF.filter(col(\"gender\") == \"M\")\n",
    "\n",
    "filteredDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b0c8718-d927-48b8-9deb-eb68853bdef6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In addition to the **Scan** (the JDBC read) we saw in the previous example, here we also see the **InMemoryTableScan** followed by a **Filter** in the explain plan.\n",
    "\n",
    "This means Spark had to read ALL the data from the database and cache it, and then scan it in cache to find the records matching the filter condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fef8b41f-95b0-4bf7-a689-e8ab5c4ac496",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Remember to clean up after ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9493d7db-1a26-4571-b8fd-760eb634e8aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cachedDF.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6484785-59bb-437b-af6c-6111be26790f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean up classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224803bd-7478-4487-8ab4-cceb511b731a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Dropped database and removed files in working directory"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Dropped database and removed files in working directory",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./Includes/Classroom-Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e477165c-04ba-4801-920d-7bdefd8b9bd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 403128964812660,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "ASP 3.2 - Query Optimization",
   "notebookOrigID": 874905206501403,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
