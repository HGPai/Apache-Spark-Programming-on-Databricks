{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b428fade-ea1e-417c-bdf4-92fd9befeb00","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Partitioning\n##### Objectives\n1. Get partitions and cores\n1. Repartition DataFrames\n1. Configure default shuffle partitions\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>: `repartition`, `coalesce`, `rdd.getNumPartitions`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html?#pyspark.SparkConf\" target=\"_blank\">SparkConf</a>: `get`, `set`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#spark-session-apis\" target=\"_blank\">SparkSession</a>: `spark.sparkContext.defaultParallelism`\n\n##### SparkConf Parameters\n- `spark.sql.shuffle.partitions`, `spark.sql.adaptive.enabled`"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"111e3052-c864-4341-a17c-89d769f42016","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5ae58b5-c647-471d-8b17-83e13d0cf52a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":[""]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Finished setting up utiltity methods...","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Finished setting up utiltity methods..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Datasets mounted and student environment set up","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Datasets mounted and student environment set up"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Get partitions and cores\n\nUse the `rdd` method `getNumPartitions` to get the number of DataFrame partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"225aa647-147b-4694-9276-7051af48d616","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.read.parquet(eventsPath)\ndf.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec3af7af-b40f-4f95-801a-6e26cdf7becf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[4]: 4","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[4]: 4"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df6e52cc-a2c9-47fc-925a-9c51e3e4cd24","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Access `SparkContext` through `SparkSession` to get the number of cores or slots.\n\nUse the `defaultParallelism` attribute to get the number of cores in a cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7107cd91-0133-4c36-8c43-85f0f479b07c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(spark.sparkContext.defaultParallelism)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d0a64f6c-dba0-498e-a5c3-0b24ca4ec9b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"8\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["`SparkContext` is also provided in Databricks notebooks as the variable `sc`."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d15e27d-5835-40f7-b665-401871ba47c5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(sc.defaultParallelism)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"91fc4030-38cd-4ccb-8dab-ff8237a7a41d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"8\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Repartition DataFrame\n\nThere are two methods available to repartition a DataFrame: `repartition` and `coalesce`."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"836e005a-4da1-4669-ad03-d6db21c90108","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### `repartition`\nReturns a new DataFrame that has exactly `n` partitions.\n\n- Wide transformation\n- Pro: Evenly balances partition sizes  \n- Con: Requires shuffling all data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"204b4392-ef4c-43d0-a48d-295bdd529a1b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["repartitionedDF = df.repartition(8)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d60a8bb0-5b1b-475c-a4bf-c90d1bcd4f2a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["repartitionedDF.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1585df3c-067c-4de3-8407-e98d360ce71e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[8]: 8","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: 8"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### `coalesce`\nReturns a new DataFrame that has exactly `n` partitions, when fewer partitions are requested.\n\nIf a larger number of partitions is requested, it will stay at the current number of partitions.\n\n- Narrow transformation, some partitions are effectively concatenated\n- Pro: Requires no shuffling\n- Cons:\n  - Is not able to increase # partitions\n  - Can result in uneven partition sizes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"57b4de2f-35dd-49c2-a4bc-a41c23e2f8b9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["coalesceDF = df.coalesce(8)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c2686f8-cfcf-42ca-bf09-e9a9e1901a88","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["coalesceDF.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f34a8f18-d380-4e36-b8bb-04687b61b4f7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[10]: 4","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[10]: 4"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Configure default shuffle partitions\n\nUse the SparkSession's `conf` attribute to get and set dynamic Spark configuration properties. The `spark.sql.shuffle.partitions` property determines the number of partitions that result from a shuffle. Let's check its default value:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e37a7bb-3cbe-4ac0-8b82-a68b23df9ec3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.shuffle.partitions\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"146a0191-6439-4c44-9efa-007da9d27aba","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[11]: '200'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: '200'"]}}],"execution_count":0},{"cell_type":"markdown","source":["Assuming that the data set isn't too large, you could configure the default number of shuffle partitions to match the number of cores:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0377dca-5d23-42d3-9655-47e059767506","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\nprint(spark.conf.get(\"spark.sql.shuffle.partitions\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"017d0e65-aff0-4fd5-b7a4-1ccaa607f1e5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"8\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Partitioning Guidelines\n- Make the number of partitions a multiple of the number of cores\n- Target a partition size of ~200MB\n- Size default shuffle partitions by dividing largest shuffle stage input by the target partition size (e.g., 4TB / 200MB = 20,000 shuffle partition count)\n\n<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> When writing a DataFrame to storage, the number of DataFrame partitions determines the number of data files written. (This assumes that <a href=\"https://sparkbyexamples.com/apache-hive/hive-partitions-explained-with-examples/\" target=\"_blank\">Hive partitioning</a> is not used for the data in storage. A discussion of DataFrame partitioning vs Hive partitioning is beyond the scope of this class.)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fd04e1c4-dd83-4a44-a0c1-5cc5d748f928","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Adaptive Query Execution\n\n<img src=\"https://files.training.databricks.com/images/aspwd/partitioning_aqe.png\" width=\"60%\" />\n\nIn Spark 3, <a href=\"https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution\" target=\"_blank\">AQE</a> is now able to <a href=\"https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html\" target=\"_blank\"> dynamically coalesce shuffle partitions</a> at runtime. This means that you can set `spark.sql.shuffle.partitions` based on the largest data set your application processes and allow AQE to reduce the number of partitions automatically when there is less data to process.\n\nThe `spark.sql.adaptive.enabled` configuration option controls whether AQE is turned on/off."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43c7f29c-ca70-432a-a395-a922f12321b5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.adaptive.enabled\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f7cd9b6a-96e9-49d9-9180-56ea8ceed1cc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[13]: 'true'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[13]: 'true'"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d73db60-54c6-40d9-9c4a-266313634ae1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f6d87b6-1097-477a-a6e4-316c8151d26e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Dropped database and removed files in working directory","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Dropped database and removed files in working directory"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nDROP DATABASE dbacademy_admin_databricks_novigosolutions_com_spark_programming_asp_3_3___partitioning cascade"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"1407402f-ec22-48cd-af0f-0811e0f48ff1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"239829f8-63a8-432b-9e60-dea4f3c880f3","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 3.3 - Partitioning","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2447284508793756,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":874905206501648}},"nbformat":4,"nbformat_minor":0}
